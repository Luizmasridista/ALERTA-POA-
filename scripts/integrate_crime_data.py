#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IntegraÃ§Ã£o de Dados de Criminalidade
Integra dados distribuÃ­dos com dataset atual do projeto
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import json
from typing import Dict, List, Tuple

class CrimeDataIntegrator:
    def __init__(self):
        self.current_data_file = "../data/distributed_crime_data.csv"
        self.distributed_data_file = "distributed_crime_data.csv"
        self.output_file = "integrated_crime_data.csv"
        
    def load_current_data(self) -> pd.DataFrame:
        """Carrega dados atuais do projeto"""
        try:
            df = pd.read_csv(self.current_data_file)
            print(f"âœ… Dados atuais carregados: {len(df)} registros")
            print(f"ğŸ“… PerÃ­odo: {df['Data Registro'].min()} a {df['Data Registro'].max()}")
            print(f"ğŸ˜ï¸  Bairros: {df['Bairro'].nunique()} Ãºnicos")
            print(f"ğŸš¨ Tipos de crime: {df['Descricao do Fato'].nunique()} Ãºnicos")
            return df
        except FileNotFoundError:
            print(f"âš ï¸  Arquivo {self.current_data_file} nÃ£o encontrado")
            return pd.DataFrame()
    
    def load_distributed_data(self) -> pd.DataFrame:
        """Carrega dados distribuÃ­dos pelo modelo"""
        try:
            df = pd.read_csv(self.distributed_data_file)
            print(f"âœ… Dados distribuÃ­dos carregados: {len(df)} registros")
            print(f"ğŸ“… PerÃ­odo: {df['data'].min()} a {df['data'].max()}")
            print(f"ğŸ˜ï¸  Bairros: {df['bairro'].nunique()} Ãºnicos")
            print(f"ğŸš¨ Tipos de crime: {df['tipo_crime'].nunique()} Ãºnicos")
            return df
        except FileNotFoundError:
            print(f"âš ï¸  Arquivo {self.distributed_data_file} nÃ£o encontrado")
            return pd.DataFrame()
    
    def standardize_current_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Padroniza dados atuais para formato comum"""
        if df.empty:
            return df
        
        # Mapear colunas para formato padrÃ£o
        standardized = pd.DataFrame()
        
        # Data
        if 'Data Registro' in df.columns:
            standardized['data'] = pd.to_datetime(df['Data Registro']).dt.strftime('%Y-%m-%d')
        elif 'Data' in df.columns:
            standardized['data'] = pd.to_datetime(df['Data']).dt.strftime('%Y-%m-%d')
        
        # Bairro
        if 'Bairro' in df.columns:
            standardized['bairro'] = df['Bairro']
        
        # Tipo de crime
        if 'Descricao do Fato' in df.columns:
            standardized['tipo_crime'] = df['Descricao do Fato']
        elif 'Tipo' in df.columns:
            standardized['tipo_crime'] = df['Tipo']
        elif 'Crime' in df.columns:
            standardized['tipo_crime'] = df['Crime']
        
        # Quantidade (assumir 1 por registro)
        standardized['quantidade'] = 1
        
        # Coordenadas se disponÃ­veis
        if 'Latitude' in df.columns and 'Longitude' in df.columns:
            standardized['latitude'] = df['Latitude']
            standardized['longitude'] = df['Longitude']
        
        # PerÃ­odo do dia se disponÃ­vel
        if 'Periodo do Dia' in df.columns:
            standardized['periodo_dia'] = df['Periodo do Dia']
        
        # Fonte
        standardized['fonte'] = 'Dataset Original'
        
        # Zona (classificar)
        standardized['zona'] = standardized['bairro'].apply(self.classify_neighborhood_zone)
        
        print(f"ğŸ“Š Dados atuais padronizados: {len(standardized)} registros")
        return standardized
    
    def classify_neighborhood_zone(self, neighborhood: str) -> str:
        """Classifica bairro por zona geogrÃ¡fica"""
        zone_mapping = {
            "Centro": ["Centro HistÃ³rico", "Cidade Baixa", "Floresta", "MarcÃ­lio Dias", 
                      "Menino Deus", "Praia de Belas", "Santa CecÃ­lia", "Azenha"],
            "Norte": ["Anchieta", "Auxiliadora", "Bom Fim", "Farroupilha", "HigienÃ³polis", 
                     "IndependÃªncia", "Moinhos de Vento", "Mont'Serrat", "Rio Branco", "Santana",
                     "Jardim LindÃ³ia", "Jardim SÃ£o Pedro", "Mapa", "Mathias Velho", "Passo d'Areia", 
                     "PetrÃ³polis", "Rubem Berta", "SÃ£o Geraldo", "SÃ£o JoÃ£o", "Sarandi", "Vila Ipiranga"],
            "Sul": ["Aberta dos Morros", "BelÃ©m Novo", "BelÃ©m Velho", "Campo Novo", "Cavalhada", 
                   "ChapÃ©u do Sol", "Coronel AparÃ­cio Borges", "EspÃ­rito Santo", "GuarujÃ¡", 
                   "HÃ­pica", "Ipanema", "Lageado", "Lami", "Nonoai", "Pedra Redonda", 
                   "Ponta Grossa", "Restinga", "Serraria", "SÃ©timo CÃ©u", "Tristeza", 
                   "Vila AssunÃ§Ã£o", "Vila ConceiÃ§Ã£o", "Vila Nova"],
            "Leste": ["Agronomia", "Bela Vista", "Boa Vista", "CamaquÃ£", "Cascata", "Cristal", 
                     "GlÃ³ria", "Jardim BotÃ¢nico", "Jardim do Salso", "Lomba do Pinheiro", 
                     "MÃ¡rio Quintana", "Medianeira", "Partenon", "Santo AntÃ´nio", "SÃ£o JosÃ©", 
                     "TrÃªs Figueiras", "Vila JoÃ£o Pessoa", "Volta do Guerino"],
            "Oeste": ["ArquipÃ©lago", "HumaitÃ¡", "Ilha da Pintada", "Ilha do PavÃ£o", "Navegantes"]
        }
        
        for zone, neighborhoods in zone_mapping.items():
            if neighborhood in neighborhoods:
                return zone
        return "Norte"  # Default
    
    def standardize_crime_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """Padroniza tipos de crimes"""
        crime_mapping = {
            # Mapeamento de crimes do dataset atual para tipos padronizados
            "ROUBO A TRANSEUNTE": "Roubo",
            "ROUBO DE VEICULO": "Roubo de veÃ­culo",
            "ROUBO EM RESIDENCIA": "Roubo",
            "ROUBO EM ESTABELECIMENTO COMERCIAL": "Roubo",
            "FURTO DE VEICULO": "Furto",
            "FURTO EM RESIDENCIA": "Furto",
            "FURTO EM ESTABELECIMENTO COMERCIAL": "Furto",
            "HOMICIDIO DOLOSO": "HomicÃ­dio",
            "LESAO CORPORAL": "LesÃ£o corporal",
            "TRAFICO DE DROGAS": "TrÃ¡fico de drogas",
            "AMEACA": "AmeaÃ§a"
        }
        
        # Aplicar mapeamento
        df['tipo_crime_padronizado'] = df['tipo_crime'].str.upper().map(crime_mapping)
        
        # Para crimes nÃ£o mapeados, manter original
        df['tipo_crime_padronizado'] = df['tipo_crime_padronizado'].fillna(df['tipo_crime'])
        
        # Substituir coluna original
        df['tipo_crime'] = df['tipo_crime_padronizado']
        df = df.drop('tipo_crime_padronizado', axis=1)
        
        return df
    
    def aggregate_current_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Agrega dados atuais por data, bairro e tipo de crime"""
        if df.empty:
            return df
        
        # Agrupar por data, bairro e tipo de crime
        aggregated = df.groupby(['data', 'bairro', 'tipo_crime', 'zona', 'fonte']).agg({
            'quantidade': 'sum',
            'latitude': 'mean',
            'longitude': 'mean',
            'periodo_dia': lambda x: x.mode().iloc[0] if not x.empty else None
        }).reset_index()
        
        print(f"ğŸ“Š Dados atuais agregados: {len(aggregated)} registros Ãºnicos")
        return aggregated
    
    def filter_overlapping_data(self, current_df: pd.DataFrame, distributed_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Remove sobreposiÃ§Ãµes entre dados atuais e distribuÃ­dos"""
        if current_df.empty or distributed_df.empty:
            return current_df, distributed_df
        
        # Identificar bairros e perÃ­odos que jÃ¡ existem nos dados atuais
        current_neighborhoods = set(current_df['bairro'].unique())
        current_dates = set(pd.to_datetime(current_df['data']).dt.to_period('M'))
        
        print(f"ğŸ˜ï¸  Bairros nos dados atuais: {len(current_neighborhoods)}")
        print(f"ğŸ“… PerÃ­odos nos dados atuais: {len(current_dates)}")
        
        # Filtrar dados distribuÃ­dos para evitar sobreposiÃ§Ã£o
        distributed_df['data_period'] = pd.to_datetime(distributed_df['data']).dt.to_period('M')
        
        # Manter apenas dados distribuÃ­dos para:
        # 1. Bairros nÃ£o cobertos pelos dados atuais
        # 2. PerÃ­odos nÃ£o cobertos pelos dados atuais para bairros existentes
        filtered_distributed = distributed_df[
            (~distributed_df['bairro'].isin(current_neighborhoods)) |
            (~distributed_df['data_period'].isin(current_dates))
        ].copy()
        
        filtered_distributed = filtered_distributed.drop('data_period', axis=1)
        
        print(f"ğŸ“Š Dados distribuÃ­dos filtrados: {len(filtered_distributed)} registros (removidas sobreposiÃ§Ãµes)")
        
        return current_df, filtered_distributed
    
    def integrate_datasets(self, current_df: pd.DataFrame, distributed_df: pd.DataFrame) -> pd.DataFrame:
        """Integra datasets atual e distribuÃ­do"""
        if current_df.empty and distributed_df.empty:
            print("âš ï¸  Nenhum dado disponÃ­vel para integraÃ§Ã£o")
            return pd.DataFrame()
        
        if current_df.empty:
            print("ğŸ“Š Usando apenas dados distribuÃ­dos")
            return distributed_df
        
        if distributed_df.empty:
            print("ğŸ“Š Usando apenas dados atuais")
            return current_df
        
        # Garantir que as colunas sejam compatÃ­veis
        common_columns = ['data', 'bairro', 'tipo_crime', 'quantidade', 'zona', 'fonte']
        
        # Adicionar colunas faltantes com valores padrÃ£o
        for col in common_columns:
            if col not in current_df.columns:
                current_df[col] = None
            if col not in distributed_df.columns:
                distributed_df[col] = None
        
        # Selecionar apenas colunas comuns
        current_df_clean = current_df[common_columns].copy()
        distributed_df_clean = distributed_df[common_columns].copy()
        
        # Combinar datasets
        integrated_df = pd.concat([current_df_clean, distributed_df_clean], ignore_index=True)
        
        # Ordenar por data e bairro
        integrated_df = integrated_df.sort_values(['data', 'bairro', 'tipo_crime'])
        
        print(f"âœ… Datasets integrados: {len(integrated_df)} registros totais")
        return integrated_df
    
    def validate_integration(self, integrated_df: pd.DataFrame) -> Dict:
        """Valida a integraÃ§Ã£o dos dados"""
        if integrated_df.empty:
            return {"error": "Dataset integrado estÃ¡ vazio"}
        
        validation = {
            "total_records": len(integrated_df),
            "unique_neighborhoods": integrated_df['bairro'].nunique(),
            "unique_crime_types": integrated_df['tipo_crime'].nunique(),
            "date_range": {
                "start": integrated_df['data'].min(),
                "end": integrated_df['data'].max()
            },
            "sources": integrated_df['fonte'].value_counts().to_dict(),
            "zones": integrated_df['zona'].value_counts().to_dict(),
            "crime_types": integrated_df['tipo_crime'].value_counts().to_dict()
        }
        
        # Verificar cobertura geogrÃ¡fica
        all_poa_neighborhoods = 94  # Total oficial
        coverage_percentage = (validation["unique_neighborhoods"] / all_poa_neighborhoods) * 100
        validation["geographic_coverage"] = {
            "covered_neighborhoods": validation["unique_neighborhoods"],
            "total_neighborhoods": all_poa_neighborhoods,
            "coverage_percentage": round(coverage_percentage, 1)
        }
        
        return validation
    
    def generate_integration_report(self, validation: Dict):
        """Gera relatÃ³rio da integraÃ§Ã£o"""
        print("=" * 80)
        print("RELATÃ“RIO DE INTEGRAÃ‡ÃƒO DE DADOS DE CRIMINALIDADE")
        print("=" * 80)
        print(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        if "error" in validation:
            print(f"âŒ Erro: {validation['error']}")
            return
        
        # EstatÃ­sticas gerais
        print("1. ESTATÃSTICAS GERAIS:")
        print("-" * 40)
        print(f"â€¢ Total de registros: {validation['total_records']:,}")
        print(f"â€¢ Bairros Ãºnicos: {validation['unique_neighborhoods']}")
        print(f"â€¢ Tipos de crime Ãºnicos: {validation['unique_crime_types']}")
        print(f"â€¢ PerÃ­odo: {validation['date_range']['start']} a {validation['date_range']['end']}")
        print()
        
        # Cobertura geogrÃ¡fica
        coverage = validation['geographic_coverage']
        print("2. COBERTURA GEOGRÃFICA:")
        print("-" * 40)
        print(f"â€¢ Bairros cobertos: {coverage['covered_neighborhoods']}/{coverage['total_neighborhoods']}")
        print(f"â€¢ Percentual de cobertura: {coverage['coverage_percentage']}%")
        print()
        
        # DistribuiÃ§Ã£o por fonte
        print("3. DISTRIBUIÃ‡ÃƒO POR FONTE:")
        print("-" * 40)
        for source, count in validation['sources'].items():
            percentage = (count / validation['total_records']) * 100
            print(f"â€¢ {source}: {count:,} registros ({percentage:.1f}%)")
        print()
        
        # DistribuiÃ§Ã£o por zona
        print("4. DISTRIBUIÃ‡ÃƒO POR ZONA:")
        print("-" * 40)
        for zone, count in validation['zones'].items():
            percentage = (count / validation['total_records']) * 100
            print(f"â€¢ Zona {zone}: {count:,} registros ({percentage:.1f}%)")
        print()
        
        # Top 10 tipos de crime
        print("5. TOP 10 TIPOS DE CRIME:")
        print("-" * 40)
        crime_types = validation['crime_types']
        sorted_crimes = sorted(crime_types.items(), key=lambda x: x[1], reverse=True)[:10]
        for i, (crime_type, count) in enumerate(sorted_crimes, 1):
            percentage = (count / validation['total_records']) * 100
            print(f"{i:2d}. {crime_type}: {count:,} registros ({percentage:.1f}%)")
        print()
        
        print("6. PRÃ“XIMOS PASSOS:")
        print("-" * 40)
        print("1. Validar dados com fontes conhecidas")
        print("2. Expandir cobertura temporal se necessÃ¡rio")
        print("3. Documentar limitaÃ§Ãµes e metodologia")
        print("4. Atualizar aplicaÃ§Ã£o com dados integrados")
        print("5. Implementar monitoramento de qualidade")
        print()
        print("=" * 80)
    
    def save_integrated_data(self, integrated_df: pd.DataFrame) -> str:
        """Salva dados integrados"""
        if integrated_df.empty:
            print("âš ï¸  Nenhum dado para salvar")
            return ""
        
        # Salvar arquivo principal
        integrated_df.to_csv(self.output_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ Dados integrados salvos em: {self.output_file}")
        
        # Salvar backup com timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"integrated_crime_data_backup_{timestamp}.csv"
        integrated_df.to_csv(backup_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ Backup salvo em: {backup_file}")
        
        return self.output_file
    
    def create_metadata_file(self, validation: Dict):
        """Cria arquivo de metadados da integraÃ§Ã£o"""
        metadata = {
            "integration_info": {
                "created_at": datetime.now().isoformat(),
                "version": "1.0",
                "description": "Dataset integrado de criminalidade de Porto Alegre"
            },
            "data_sources": {
                "original_dataset": {
                    "file": self.current_data_file,
                    "description": "Dados originais do projeto ALERTA POA"
                },
                "distributed_model": {
                    "file": self.distributed_data_file,
                    "description": "Dados distribuÃ­dos baseados no modelo UFRGS"
                }
            },
            "statistics": validation,
            "limitations": [
                "Dados distribuÃ­dos sÃ£o estimativas baseadas em modelo acadÃªmico",
                "Pesquisa UFRGS Ã© de 2016-2017 (pode estar desatualizada)",
                "Alguns tipos de crime podem ter distribuiÃ§Ã£o imprecisa",
                "Fatores populacionais sÃ£o estimativas"
            ],
            "methodology": [
                "Dados originais mantidos como fonte primÃ¡ria",
                "Modelo UFRGS aplicado para distribuir dados municipais",
                "SobreposiÃ§Ãµes removidas para evitar duplicaÃ§Ã£o",
                "Tipos de crime padronizados"
            ]
        }
        
        metadata_file = "integrated_crime_data_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ Metadados salvos em: {metadata_file}")
        return metadata_file

def main():
    """FunÃ§Ã£o principal"""
    print("ğŸ”„ Iniciando integraÃ§Ã£o de dados de criminalidade...")
    
    # Criar integrador
    integrator = CrimeDataIntegrator()
    
    # Carregar dados
    print("\nğŸ“‚ Carregando dados...")
    current_data = integrator.load_current_data()
    distributed_data = integrator.load_distributed_data()
    
    # Padronizar dados atuais
    print("\nğŸ”§ Padronizando dados atuais...")
    current_standardized = integrator.standardize_current_data(current_data)
    current_standardized = integrator.standardize_crime_types(current_standardized)
    current_aggregated = integrator.aggregate_current_data(current_standardized)
    
    # Padronizar dados distribuÃ­dos
    print("\nğŸ”§ Padronizando dados distribuÃ­dos...")
    distributed_standardized = integrator.standardize_crime_types(distributed_data)
    
    # Filtrar sobreposiÃ§Ãµes
    print("\nğŸ” Filtrando sobreposiÃ§Ãµes...")
    current_filtered, distributed_filtered = integrator.filter_overlapping_data(
        current_aggregated, distributed_standardized
    )
    
    # Integrar datasets
    print("\nğŸ”— Integrando datasets...")
    integrated_data = integrator.integrate_datasets(current_filtered, distributed_filtered)
    
    # Validar integraÃ§Ã£o
    print("\nâœ… Validando integraÃ§Ã£o...")
    validation = integrator.validate_integration(integrated_data)
    
    # Gerar relatÃ³rio
    integrator.generate_integration_report(validation)
    
    # Salvar dados
    print("\nğŸ’¾ Salvando dados integrados...")
    output_file = integrator.save_integrated_data(integrated_data)
    metadata_file = integrator.create_metadata_file(validation)
    
    print(f"\nâœ… IntegraÃ§Ã£o concluÃ­da com sucesso!")
    print(f"ğŸ“ Arquivo principal: {output_file}")
    print(f"ğŸ“‹ Metadados: {metadata_file}")
    print(f"ğŸ¯ PrÃ³ximo passo: Validar com dados conhecidos")

if __name__ == "__main__":
    main()